{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from sklearn import metrics\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(P, labels, plot_tile, s) :\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    plt.scatter(P[:,0], P[:,1], c = labels, cmap = 'rainbow_r', s = s)\n",
    "    plt.gca().set_aspect('equal'), plt.title(plot_tile), plt.xlabel('x(mm)'), plt.ylabel('y(mm)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_graph(G, P, plot_title, node_size):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    nx.draw(G, P, node_size=[node_size for v in range(P.shape[0])], node_color='lightgreen'\n",
    "            ,edgelist=edges, edge_color=weights, width=5, edge_cmap=plt.cm.Blues)\n",
    "    plt.title(plot_title)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_with_graph(G, P, labels, plot_title, node_size):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    edges, weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    nodes = [i for i in G.nodes()]\n",
    "    e = []\n",
    "    w = []\n",
    "\n",
    "    for i, edge in enumerate(edges):\n",
    "        p1 = edge[0]\n",
    "        p2 = edge[1]\n",
    "        if labels[p1] == labels[p2] :\n",
    "            e.append((p1, p2))\n",
    "            w.append(1000)\n",
    "\n",
    "    nx.draw(G, P, node_size=[node_size for v in range(P.shape[0])], node_color=labels[nodes], \n",
    "            cmap=plt.cm.rainbow_r ,edgelist=e, edge_color=w, width=1)\n",
    "    plt.title(plot_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(P, Sigma, sigma_s, noise_thrsh, plot_den_hist) :\n",
    "    # Inputs : \n",
    "    # P_i           : [x, y], the localization centers\n",
    "    # Sigma_i       : [[sigma_x_i, 0], [0, sigma_y_i]] where sigma_x_i and sigma_y_i are \n",
    "    # uncertinties in x and y direction  of the P_i localization respectively.\n",
    "    # sigma_s       : scaling parameter\n",
    "    # noise_thrsh   : threshold used for denoising\n",
    "    # plot_den_hist : 0 or 1, 1 for when you want the density histogram plot\n",
    "    \n",
    "    n_points = P.shape[0]               # number of points\n",
    "    sg2      = sigma_s ** 2 \n",
    "    \n",
    "    # ------- Constructing weighted graph -------\n",
    "    dt = Delaunay(points=P)             # Delaunay triangulation for input points\n",
    "    G  = nx.Graph()                     # G will be the weighted graph for the data\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    for path in dt.simplices:\n",
    "        a  = path[0]\n",
    "        b  = path[1]\n",
    "        c  = path[2]\n",
    "        d1 = np.exp(-(distance.euclidean(P[a], P[b])**2 + np.trace(Sigma[a] + Sigma[b]))/(2*sg2))\n",
    "        d2 = np.exp(-(distance.euclidean(P[a], P[c])**2 + np.trace(Sigma[a] + Sigma[c]))/(2*sg2))\n",
    "        d3 = np.exp(-(distance.euclidean(P[b], P[c])**2 + np.trace(Sigma[b] + Sigma[c]))/(2*sg2))\n",
    "        G.add_weighted_edges_from([(a, b, d1)])\n",
    "        G.add_weighted_edges_from([(a, c, d2)])\n",
    "        G.add_weighted_edges_from([(b, c, d3)])\n",
    "    \n",
    "    # ------- Denosing -------\n",
    "    degrees          = G.degree(weight=None)\n",
    "    n_neighbors      = np.array([i for (_,i) in degrees]).ravel()\n",
    "    weights          = G.degree(weight='weight')\n",
    "    weighted_degrees = np.array([i for (_,i) in weights]).ravel()\n",
    "    density          = np.nan_to_num(weighted_degrees/n_neighbors)\n",
    "\n",
    "    not_noise_points = np.where(density>noise_thrsh)[0]\n",
    "    noise_points     = np.where(density<=noise_thrsh)[0]\n",
    "    \n",
    "    # ------- Denoising with kmeans - not in the paper -------\n",
    "#     den_kmeans = KMeans(n_clusters=3, random_state=0, n_init = 20).fit(density.reshape(-1, 1))\n",
    "#     den_groups       = den_kmeans.labels_\n",
    "#     den_groups_means = np.array([np.mean(density[np.where(den_groups==i)]) for i in range(3)])\n",
    "#     min_density      = np.where(den_groups==min(den_groups))[0]\n",
    "#     not_noise_points = np.where(den_groups!=min_density)[0]\n",
    "#     noise_points     = np.where(den_groups==min_density)[0]\n",
    "    \n",
    "    if plot_den_hist:\n",
    "        plt.hist(density, bins=100)\n",
    "        plt.title('histogram of node density -all-')\n",
    "        plt.show()\n",
    "        \n",
    "    return density, noise_points, not_noise_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels : set at domain decomposition step\n",
    "\n",
    "def cluster_single_scale(P, Sigma, not_noise_point, sigma_s, plot_eigen) :\n",
    "    # Inputs : \n",
    "    # P_i : [x, y], the localization centers\n",
    "    # Sigma_i : [[sigma_x_i, 0], [0, sigma_y_i]] where sigma_x_i and sigma_y_i are \n",
    "    # uncertinties in x and y direction  of the P_i localization respectively.\n",
    "    # sigma_s : scaling parameter\n",
    "    # not_noise_point : index of not noise points\n",
    "    # plot_eigen : 0 or 1, 1 if you want eigenvalues and eigenvalue differences plot\n",
    "\n",
    "    n_points_wn = P.shape[0]\n",
    "    P           = P[not_noise_point]\n",
    "    Sigma       = Sigma[not_noise_point]\n",
    "    n_points    = P.shape[0]               # number of non-noise points\n",
    "    sg2         = sigma_s ** 2 \n",
    "    \n",
    "    if n_points < 3 :\n",
    "        print('Not enough points!')\n",
    "        return\n",
    "    \n",
    "    # ------- Constructing weighted graph -------\n",
    "    dt = Delaunay(points=P)             # Delaunay triangulation for input points\n",
    "    G  = nx.Graph()                     # G will be the weighted graph for the data\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    for path in dt.simplices:\n",
    "        a  = path[0]\n",
    "        b  = path[1]\n",
    "        c  = path[2]\n",
    "        d1 = np.exp(-(distance.euclidean(P[a], P[b])**2 + np.trace(Sigma[a] + Sigma[b]))/(2*sg2))\n",
    "        d2 = np.exp(-(distance.euclidean(P[a], P[c])**2 + np.trace(Sigma[a] + Sigma[c]))/(2*sg2))\n",
    "        d3 = np.exp(-(distance.euclidean(P[b], P[c])**2 + np.trace(Sigma[b] + Sigma[c]))/(2*sg2))\n",
    "        G.add_weighted_edges_from([(a, b, d1)])\n",
    "        G.add_weighted_edges_from([(a, c, d2)])\n",
    "        G.add_weighted_edges_from([(b, c, d3)])\n",
    "        \n",
    "    # ------- Spectral Clustering -------\n",
    "    \n",
    "    # ------- 1. Calculating eigenvalues and eigenvectors -------\n",
    "    vals, vecs = eigsh(nx.laplacian_matrix(G), k=n_points-1, which='SM')\n",
    "                    \n",
    "    # ------- 2. Determining number of clusters -------\n",
    "    \n",
    "    diff_vals  = np.diff(vals[0:int(n_points/2)])\n",
    "    prominence = np.max(diff_vals) / 4\n",
    "    k          = np.min(np.where(diff_vals >= prominence)[0]) + 1\n",
    "    \n",
    "    if vals[1] > prominence:   # handle only 1 cluster case\n",
    "        k = 1\n",
    "        \n",
    "    if plot_eigen:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "        axs[0].scatter(list(range(len(diff_vals[0:k+5]))), diff_vals[0:k+5])\n",
    "        axs[0].set_title('Eigenvalue Differences')\n",
    "        axs[1].scatter(list(range(len(vals[0:k+5]))), vals[0:k+5])\n",
    "        axs[1].set_title('Eigenvalues')\n",
    "        plt.show()\n",
    "        \n",
    "    # ------- 3. Kmeans -------\n",
    "    kmeans_spec = KMeans(n_clusters=k, random_state=0, n_init = 20).fit(vecs[:,0:k])\n",
    "    labels_not_noise = kmeans_spec.labels_\n",
    "    \n",
    "    labels                  = np.zeros((n_points_wn, 1)).ravel() - 1 # -1 is label for noise\n",
    "    labels[not_noise_point] = labels_not_noise\n",
    "    \n",
    "    return labels, G, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_to_point(P, Sigma, n, k, labels) :\n",
    "    # This function turns clusters into their respresentitive point of the next scale\n",
    "    # Inputs : \n",
    "    # P_i : [x, y], the localization centers\n",
    "    # Sigma_i : [[sigma_x_i, 0], [0, sigma_y_i]] where sigma_x_i and sigma_y_i are \n",
    "    # uncertinties in x and y direction  of the P_i localization respectively.\n",
    "    # n_i : number of photons in P_i th detection\n",
    "    \n",
    "    P_next      = np.zeros((k, 2))\n",
    "    Sigma_next  = np.zeros((k, 2, 2))\n",
    "    n_next      = np.zeros((k, 1)).ravel()\n",
    "    cluster_id  = np.zeros((k, 1))\n",
    "\n",
    "    for i, cluster_num in enumerate(np.unique(labels)):\n",
    "        idx_cluster = np.where(labels==cluster_num)[0]\n",
    "        mx          = np.average(P[idx_cluster, 0], axis=0, weights=n[idx_cluster])\n",
    "        my          = np.average(P[idx_cluster, 1], axis=0, weights=n[idx_cluster])\n",
    "        sx          = 0\n",
    "        sy          = 0\n",
    "        sxy         = 0\n",
    "        \n",
    "        for idx in idx_cluster : \n",
    "            sx  = (P[idx,0]-mx) ** 2 * n[idx] + sx\n",
    "            sy  = (P[idx,1]-my) ** 2 * n[idx] + sy\n",
    "            sxy = (P[idx,0]-mx) * (P[idx,1]-my) * n[idx] + sxy\n",
    "            \n",
    "        sx  = sx  / (np.sum(n[idx_cluster]))\n",
    "        sy  = sy  / (np.sum(n[idx_cluster]))\n",
    "        sxy = sxy / (np.sum(n[idx_cluster]))\n",
    "        \n",
    "        P_next[i]      = [mx, my]\n",
    "        Sigma_next[i]  = [[sx, sxy], [sxy, sy]]\n",
    "        n_next[i]      = np.sum(n[idx_cluster])\n",
    "        cluster_id[i]  = cluster_num \n",
    "        \n",
    "    return P_next, Sigma_next, n_next, cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_input(labels1_fid, labels2, k2, cluster_id2):\n",
    "    # This function maps clustering labels of a specific scale data to the input data\n",
    "    # Inputs:\n",
    "    # labels1_fid : labels of the previous scale for the input data\n",
    "    # labels2 : labels of data clustered at the current scale\n",
    "    # k2 : number of the clusters in the current scale\n",
    "\n",
    "    labels2_fid     = labels1_fid.copy()\n",
    "    counter         = k2\n",
    "    \n",
    "    for lbl in np.unique(labels2):\n",
    "        idx_ns = cluster_id2[np.where(labels2==lbl)[0]]\n",
    "\n",
    "        if lbl == -1:\n",
    "            for idx in idx_ns:\n",
    "                idx_fs              = np.where(labels1_fid==idx)[0]\n",
    "                labels2_fid[idx_fs] = counter\n",
    "                counter = counter + 1\n",
    "\n",
    "        else:\n",
    "            for idx in idx_ns:\n",
    "                idx_fs              = np.where(labels1_fid==idx)[0]\n",
    "                labels2_fid[idx_fs] = lbl\n",
    "            \n",
    "    return labels2_fid, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/data0_0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fb7fec38ca33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# save data and truelabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/data\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/truelabel\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruelabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0;31m# datasource doesn't support creating a new file ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mown_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/data0_0.csv'"
     ]
    }
   ],
   "source": [
    "# Generate test data and calculate DBscan and GraphHic metrics\n",
    "\n",
    "def give_circle_points(rmin, rmax, n_points, center):\n",
    "    r         = np.random.uniform(rmin, rmax, 1)\n",
    "    r_s       = np.random.uniform(0, r, n_points)\n",
    "    theta_s   = np.random.uniform(0, np.pi*2, n_points)\n",
    "    sin_theta = np.sin(theta_s)\n",
    "    cos_theta = np.cos(theta_s)\n",
    "    points    = [[center[0] + r_s[i] * cos_theta[i], center[1] + r_s[i] * sin_theta[i]] for i in range(n_points)]  \n",
    "    return points\n",
    "\n",
    "for r_seed in range(50):\n",
    "    print(r_seed)\n",
    "\n",
    "    for i in range(14):\n",
    "        np.random.seed(r_seed)\n",
    "        \n",
    "        centers    = [(0, 0), (0, 50), (50, 0), (50, 50)]\n",
    "        points     = np.array([[0, 0]])\n",
    "        truelabels = []\n",
    "        \n",
    "        q       = 1 - i * 0.05\n",
    "        n1      = 500\n",
    "        numbers = [n1, int(q*n1), int(q**2*n1), int(q**3*n1)]\n",
    "        k       = 0\n",
    "        \n",
    "        for center_num, center in enumerate(centers):\n",
    "            temp       = give_circle_points(23, 23, numbers[center_num], center)\n",
    "            truelabels = truelabels + [k for t in list(range(numbers[center_num]))]\n",
    "            k          = k + 1\n",
    "            for t in temp:\n",
    "                points = np.append(points, [t], axis=0)\n",
    "                \n",
    "                \n",
    "        points     = points[1:]\n",
    "        nData      = points.shape[0]\n",
    "        noise      = np.random.uniform(low=[-50, -50], high=[100, 100], size=(75,2))\n",
    "        truelabels = truelabels + [k for t in list(range(75))]\n",
    "        k          = k + 1\n",
    "        for t in noise:\n",
    "            points = np.append(points, [t], axis=0)\n",
    "\n",
    "        truelabels     = np.array(truelabels)\n",
    "        P              = points \n",
    "        Sigma          = np.ones((P.shape[0], 2, 2)) * 0.5\n",
    "        Sigma[:, 0, 1] = 0\n",
    "        Sigma[:, 1, 0] = 0\n",
    "        n              = np.ones((P.shape[0], 1)).ravel()\n",
    "        \n",
    "        # save data and truelabels\n",
    "        np.savetxt(\"Data/data\"+str(int(r_seed)) +\"_\" +str(i) + \".csv\", P, delimiter=\",\")\n",
    "        np.savetxt(\"Data/truelabel\"+str(int(r_seed)) +\"_\" +str(i) + \".csv\", truelabels, delimiter=\",\")\n",
    "\n",
    "        # DBSCAN\n",
    "        db                = DBSCAN(eps=4, min_samples=5).fit(P)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels_db         = db.labels_\n",
    "        \n",
    "        np.savetxt(\"Data/dbscan\"+str(int(r_seed)) +\"_\" +str(i) + \".csv\", labels_db, delimiter=\",\")\n",
    "        \n",
    "        # GraphHiC\n",
    "        density, noise_points, not_noise_points = denoise(P, Sigma, 4.5, 0.15, 0)\n",
    "        labels_sc, G, k = cluster_single_scale(P, Sigma, not_noise_points, 4.5, 0)\n",
    "\n",
    "        np.savetxt(\"Data/graphic\"+str(int(r_seed)) +\"_\" +str(i) + \".csv\", labels_sc, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
