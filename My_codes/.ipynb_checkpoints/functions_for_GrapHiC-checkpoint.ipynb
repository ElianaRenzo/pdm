{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from sklearn import metrics\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(P, labels, plot_tile, s) :\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.scatter(P[:,0], P[:,1], c = labels, cmap = 'rainbow_r', s = s)\n",
    "    plt.gca().set_aspect('equal'), plt.title(plot_tile), plt.xlabel('x(nm)'), plt.ylabel('y(nm)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig_pdf(P, labels, path, filename, s):\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    plt.scatter(P[:,0], P[:,1], c = labels, cmap = 'rainbow_r', s = s)\n",
    "    plt.gca().set_aspect('equal'), plt.title(filename), plt.xlabel('x(nm)'), plt.ylabel('y(nm)')\n",
    "    plt.savefig(path + filename + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_graph(G, P, plot_title, node_size):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    nx.draw(G, P, node_size=[node_size for v in range(P.shape[0])], node_color='lightgreen'\n",
    "            ,edgelist=edges, edge_color=weights, width=5, edge_cmap=plt.cm.Blues)\n",
    "    plt.title(plot_title)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_with_graph(G, P, labels, plot_title, node_size):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    edges, weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    nodes = [i for i in G.nodes()]\n",
    "    e = []\n",
    "    w = []\n",
    "\n",
    "    for i, edge in enumerate(edges):\n",
    "        p1 = edge[0]\n",
    "        p2 = edge[1]\n",
    "        if labels[p1] == labels[p2] :\n",
    "            e.append((p1, p2))\n",
    "            w.append(1000)\n",
    "\n",
    "    nx.draw(G, P, node_size=[node_size for v in range(P.shape[0])], node_color=labels[nodes], \n",
    "            cmap=plt.cm.rainbow_r ,edgelist=e, edge_color=w, width=1)\n",
    "    plt.title(plot_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(P, Sigma, sigma_s, noise_thrsh, plot_den_hist) :\n",
    "    # Inputs : \n",
    "    # P_i           : [x, y], the localization centers\n",
    "    # Sigma_i       : [[sigma_x_i, 0], [0, sigma_y_i]] where sigma_x_i and sigma_y_i are \n",
    "    # uncertinties in x and y direction  of the P_i localization respectively.\n",
    "    # sigma_s       : scaling parameter\n",
    "    # noise_thrsh   : threshold used for denoising\n",
    "    # plot_den_hist : 0 or 1, 1 for when you want the density histogram plot\n",
    "    \n",
    "    n_points = P.shape[0]               # number of points\n",
    "    sg2      = sigma_s ** 2 \n",
    "    \n",
    "    # ------- Constructing weighted graph -------\n",
    "    dt = Delaunay(points=P)             # Delaunay triangulation for input points\n",
    "    G  = nx.Graph()                     # G will be the weighted graph for the data\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    for path in dt.simplices:\n",
    "        a  = path[0]\n",
    "        b  = path[1]\n",
    "        c  = path[2]\n",
    "        d1 = np.exp(-(wasserstein_distance_2(P[a], P[b], Sigma[a], Sigma[b])) /(2*sg2))\n",
    "        d2 = np.exp(-(wasserstein_distance_2(P[a], P[c], Sigma[a], Sigma[c])) /(2*sg2))\n",
    "        d3 = np.exp(-(wasserstein_distance_2(P[b], P[c], Sigma[b], Sigma[c])) /(2*sg2))\n",
    "        #d1 = np.exp(-(euclidian_distance_2(P[a], P[b], Sigma[a], Sigma[b])) /(2*sg2))\n",
    "        #d2 = np.exp(-(euclidian_distance_2(P[a], P[c], Sigma[a], Sigma[c])) /(2*sg2))\n",
    "        #d3 = np.exp(-(euclidian_distance_2(P[b], P[c], Sigma[b], Sigma[c])) /(2*sg2))\n",
    "        \n",
    "        G.add_weighted_edges_from([(a, b, d1)])\n",
    "        G.add_weighted_edges_from([(a, c, d2)])\n",
    "        G.add_weighted_edges_from([(b, c, d3)])\n",
    "        \n",
    "    \n",
    "    G0 = G.copy()\n",
    "    # ------- Denosing -------\n",
    "    degrees          = G.degree(weight=None)\n",
    "    n_neighbors      = np.array([i for (_,i) in degrees]).ravel()\n",
    "    weights          = G.degree(weight='weight')\n",
    "    weighted_degrees = np.array([i for (_,i) in weights]).ravel()\n",
    "    density          = np.nan_to_num(weighted_degrees/n_neighbors)\n",
    "\n",
    "    not_noise_points = np.where(density>noise_thrsh)[0]\n",
    "    noise_points     = np.where(density<=noise_thrsh)[0]\n",
    "    \n",
    "    # ------- Denoising with kmeans - not in the paper -------\n",
    "#     den_kmeans = KMeans(n_clusters=3, random_state=0, n_init = 20).fit(density.reshape(-1, 1))\n",
    "#     den_groups       = den_kmeans.labels_\n",
    "#     den_groups_means = np.array([np.mean(density[np.where(den_groups==i)]) for i in range(3)])\n",
    "#     min_density      = np.where(den_groups==min(den_groups))[0]\n",
    "#     not_noise_points = np.where(den_groups!=min_density)[0]\n",
    "#     noise_points     = np.where(den_groups==min_density)[0]\n",
    "    \n",
    "    if plot_den_hist:\n",
    "        #plt.hist(density, bins=100)\n",
    "        plt.figure(figsize = (7,5))\n",
    "        plt.hist(density, bins=50)\n",
    "        plt.axvline(x=noise_thrsh, label='T = ' + '%.3f'%(noise_thrsh), c='DarkOrange', linewidth=3)\n",
    "        plt.xlabel(r'$\\rho$')\n",
    "        plt.ylabel('# nodes')\n",
    "        plt.xlim(-0.05,1.05)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return density, noise_points, not_noise_points, G0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_T_95(N, ListOfCovMatrices, sigma_s = 40.0, x_lim = [0,4], y_lim = [0,4], ReturnFig = False):\n",
    "    csr = CsrGenerator(N=N, x_lim = x_lim, y_lim = y_lim)\n",
    "    data = csr.GetAllData()\n",
    "    P = data[['x', 'y']].to_numpy() # select the coordinates columns, and transform into a numpy object\n",
    "    truelabels = data['labels_1'].to_numpy()\n",
    "    \n",
    "    #Sigma1          = np.ones((P.shape[0], 2, 2)) * 400.\n",
    "    #Sigma1[:, 0, 1] = 0\n",
    "    #Sigma1[:, 1, 0] = 0\n",
    "    n1              = np.ones((P.shape[0], 15000)).ravel()\n",
    "    \n",
    "    Sigma           = np.asarray(ListOfCovMatrices)\n",
    "    \n",
    "    density_csr, G0 = compute_density(P, Sigma, sigma_s)\n",
    "    \n",
    "    # Computing T \n",
    "    T = np.quantile(density_csr, 0.95)\n",
    "        \n",
    "    #see_histogram(density_csr, T, '')\n",
    "    \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_density(P, Sigma, sigma_s):\n",
    "    n_points = P.shape[0]               # number of points\n",
    "    sg2      = sigma_s ** 2 \n",
    "    \n",
    "    # ------- Constructing weighted graph -------\n",
    "    dt = Delaunay(points=P)             # Delaunay triangulation for input points\n",
    "    G  = nx.Graph()                     # G will be the weighted graph for the data\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    for path in dt.simplices:\n",
    "        a  = path[0]\n",
    "        b  = path[1]\n",
    "        c  = path[2]\n",
    "        d1 = np.exp(-(wasserstein_distance_2(P[a], P[b], Sigma[a], Sigma[b])) /(2*sg2))\n",
    "        d2 = np.exp(-(wasserstein_distance_2(P[a], P[c], Sigma[a], Sigma[c])) /(2*sg2))\n",
    "        d3 = np.exp(-(wasserstein_distance_2(P[b], P[c], Sigma[b], Sigma[c])) /(2*sg2))\n",
    "        #d1 = np.exp(-(euclidian_distance_2(P[a], P[b], Sigma[a], Sigma[b])) /(2*sg2))\n",
    "        #d2 = np.exp(-(euclidian_distance_2(P[a], P[c], Sigma[a], Sigma[c])) /(2*sg2))\n",
    "        #d3 = np.exp(-(euclidian_distance_2(P[b], P[c], Sigma[b], Sigma[c])) /(2*sg2))\n",
    "        G.add_weighted_edges_from([(a, b, d1)])\n",
    "        G.add_weighted_edges_from([(a, c, d2)])\n",
    "        G.add_weighted_edges_from([(b, c, d3)])\n",
    "        \n",
    "    \n",
    "    G0 = G.copy()\n",
    "    # ------- Denosing -------\n",
    "    degrees          = G.degree(weight=None)\n",
    "    n_neighbors      = np.array([i for (_,i) in degrees]).ravel()\n",
    "    weights          = G.degree(weight='weight')\n",
    "    weighted_degrees = np.array([i for (_,i) in weights]).ravel()\n",
    "    density          = np.nan_to_num(weighted_degrees/n_neighbors)\n",
    "\n",
    "    return density, G0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_histogram(density, noise_thrsh, title = 'histogram of node density -all-'):\n",
    "\n",
    "    #plt.hist(density, bins=100)\n",
    "    plt.figure(figsize = (7,5))\n",
    "    plt.hist(density, bins=50)\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.axvline(x=noise_thrsh, label='T = ' + '%.3f'%(noise_thrsh), c='DarkOrange', linewidth=3)\n",
    "    plt.legend()\n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.xlabel(r'$\\rho$')\n",
    "    plt.ylabel('# nodes')\n",
    "    #if show == True: plt.show()\n",
    "    saving_path = '/Users/Eliana/Documents/PDM/Codes/My_codes/automatic_T/'\n",
    "    #plt.savefig(saving_path + 'histo_with_T'+ '.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_distance_2(P1, P2, Sigma1, Sigma2):\n",
    "    S2_sqrt = scipy.linalg.sqrtm(Sigma2)\n",
    "    w_d = distance.euclidean(P1, P2)**2 + \\\n",
    "          np.trace(\n",
    "                Sigma1 + Sigma2 - 2 * scipy.linalg.sqrtm((np.dot(np.dot(S2_sqrt,\n",
    "                                                                        Sigma1),\n",
    "                                                                    S2_sqrt)))\n",
    "          )  \n",
    "    return w_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance_2(P1, P2, Sigma1, Sigma2):\n",
    "    S2_sqrt = scipy.linalg.sqrtm(Sigma2)\n",
    "    e_d = distance.euclidean(P1, P2)**2 + \\\n",
    "          np.trace(\n",
    "                Sigma1 + Sigma2\n",
    "          )  \n",
    "    return e_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels : set at domain decomposition step\n",
    "\n",
    "def cluster_single_scale(P, Sigma, not_noise_point, sigma_s, plot_eigen) :\n",
    "    # Inputs : \n",
    "    # P_i : [x, y], the localization centers\n",
    "    # Sigma_i : [[sigma_x_i, 0], [0, sigma_y_i]] where sigma_x_i and sigma_y_i are \n",
    "    # uncertinties in x and y direction  of the P_i localization respectively.\n",
    "    # sigma_s : scaling parameter\n",
    "    # not_noise_point : index of not noise points\n",
    "    # plot_eigen : 0 or 1, 1 if you want eigenvalues and eigenvalue differences plot\n",
    "\n",
    "    n_points_wn = P.shape[0]\n",
    "    P           = P[not_noise_point]\n",
    "    Sigma       = Sigma[not_noise_point]\n",
    "    n_points    = P.shape[0]               # number of non-noise points\n",
    "    sg2         = sigma_s ** 2 \n",
    "    \n",
    "    if n_points < 3 :\n",
    "        print('Not enough points!')\n",
    "        return\n",
    "    \n",
    "    # ------- Constructing weighted graph -------\n",
    "    dt = Delaunay(points=P)             # Delaunay triangulation for input points\n",
    "    G  = nx.Graph()                     # G will be the weighted graph for the data\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    for path in dt.simplices:\n",
    "        a  = path[0]\n",
    "        b  = path[1]\n",
    "        c  = path[2]\n",
    "        d1 = np.exp(-(wasserstein_distance_2(P[a], P[b], Sigma[a], Sigma[b])) /(2*sg2))\n",
    "        d2 = np.exp(-(wasserstein_distance_2(P[a], P[c], Sigma[a], Sigma[c])) /(2*sg2))\n",
    "        d3 = np.exp(-(wasserstein_distance_2(P[b], P[c], Sigma[b], Sigma[c])) /(2*sg2))\n",
    "        #d1 = np.exp(-(euclidian_distance_2(P[a], P[b], Sigma[a], Sigma[b])) /(2*sg2))\n",
    "        #d2 = np.exp(-(euclidian_distance_2(P[a], P[c], Sigma[a], Sigma[c])) /(2*sg2))\n",
    "        #d3 = np.exp(-(euclidian_distance_2(P[b], P[c], Sigma[b], Sigma[c])) /(2*sg2))\n",
    "        G.add_weighted_edges_from([(a, b, d1)])\n",
    "        G.add_weighted_edges_from([(a, c, d2)])\n",
    "        G.add_weighted_edges_from([(b, c, d3)])\n",
    "        \n",
    "    # ------- Spectral Clustering -------\n",
    "    \n",
    "    # ------- 1. Calculating eigenvalues and eigenvectors -------\n",
    "    vals, vecs = eigsh(nx.laplacian_matrix(G), k=n_points-1, which='SM')\n",
    "                    \n",
    "    # ------- 2. Determining number of clusters -------\n",
    "    \n",
    "    diff_vals  = np.diff(vals[0:int(n_points/2)])\n",
    "    prominence = np.max(diff_vals) / 4 # je crois que ici c'est le threshold pour renvoyer toutes les petites à 0.\n",
    "    #prominence = np.max(diff_vals) / 8 # je crois que ici c'est le threshold pour renvoyer toutes les petites à 0.\n",
    "    k          = np.min(np.where(diff_vals >= prominence)[0]) + 1\n",
    "    \n",
    "    if vals[1] > prominence:   # handle only 1 cluster case\n",
    "        k = 1\n",
    "        \n",
    "    if plot_eigen:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "        axs[0].scatter(list(range(len(diff_vals[0:k+5]))), diff_vals[0:k+5])\n",
    "        axs[0].set_title('Eigenvalue Differences')\n",
    "        axs[1].scatter(list(range(len(vals[0:k+5]))), vals[0:k+5])\n",
    "        axs[1].set_title('Eigenvalues')\n",
    "        plt.show()\n",
    "        \n",
    "    # ------- 3. Kmeans -------\n",
    "    kmeans_spec = KMeans(n_clusters=k, random_state=0, n_init = 20).fit(vecs[:,0:k])\n",
    "    labels_not_noise = kmeans_spec.labels_\n",
    "    \n",
    "    labels                  = np.zeros((n_points_wn, 1)).ravel() - 1 # -1 is label for noise\n",
    "    labels[not_noise_point] = labels_not_noise\n",
    "    \n",
    "    return labels, G, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_to_point(P, Sigma, n, k, labels) :\n",
    "    # This function turns clusters into their respresentitive point of the next scale\n",
    "    # Inputs : \n",
    "    # P_i : [x, y], the localization centers\n",
    "    # Sigma_i : [[sigma_x_i, 0], [0, sigma_y_i]] where sigma_x_i and sigma_y_i are \n",
    "    # uncertinties in x and y direction  of the P_i localization respectively.\n",
    "    # n_i : number of photons in P_i th detection\n",
    "    \n",
    "    P_next      = np.zeros((k, 2))\n",
    "    Sigma_next  = np.zeros((k, 2, 2))\n",
    "    n_next      = np.zeros((k, 1)).ravel()\n",
    "    cluster_id  = np.zeros((k, 1))\n",
    "\n",
    "    for i, cluster_num in enumerate(np.unique(labels)):\n",
    "        idx_cluster = np.where(labels==cluster_num)[0]\n",
    "        mx          = np.average(P[idx_cluster, 0], axis=0, weights=n[idx_cluster])\n",
    "        my          = np.average(P[idx_cluster, 1], axis=0, weights=n[idx_cluster])\n",
    "        sx          = 0\n",
    "        sy          = 0\n",
    "        sxy         = 0\n",
    "        \n",
    "        for idx in idx_cluster : \n",
    "            sx  = (P[idx,0]-mx) ** 2 * n[idx] + sx\n",
    "            sy  = (P[idx,1]-my) ** 2 * n[idx] + sy\n",
    "            sxy = (P[idx,0]-mx) * (P[idx,1]-my) * n[idx] + sxy\n",
    "            \n",
    "        sx  = sx  / (np.sum(n[idx_cluster]))\n",
    "        sy  = sy  / (np.sum(n[idx_cluster]))\n",
    "        sxy = sxy / (np.sum(n[idx_cluster]))\n",
    "        \n",
    "        P_next[i]      = [mx, my]\n",
    "        Sigma_next[i]  = [[sx, sxy], [sxy, sy]]\n",
    "        n_next[i]      = np.sum(n[idx_cluster])\n",
    "        cluster_id[i]  = cluster_num \n",
    "        \n",
    "    return P_next, Sigma_next, n_next, cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_input(labels1_fid, labels2, k2, cluster_id2):\n",
    "    # This function maps clustering labels of a specific scale data to the input data\n",
    "    # Inputs:\n",
    "    # labels1_fid : labels of the previous scale for the input data\n",
    "    # labels2 : labels of data clustered at the current scale\n",
    "    # k2 : number of the clusters in the current scale\n",
    "\n",
    "    labels2_fid     = labels1_fid.copy()\n",
    "    counter         = k2\n",
    "    \n",
    "    for lbl in np.unique(labels2):\n",
    "        idx_ns = cluster_id2[np.where(labels2==lbl)[0]]\n",
    "\n",
    "        if lbl == -1:\n",
    "            for idx in idx_ns:\n",
    "                idx_fs              = np.where(labels1_fid==idx)[0]\n",
    "                labels2_fid[idx_fs] = counter\n",
    "                counter = counter + 1\n",
    "\n",
    "        else:\n",
    "            for idx in idx_ns:\n",
    "                idx_fs              = np.where(labels1_fid==idx)[0]\n",
    "                labels2_fid[idx_fs] = lbl\n",
    "            \n",
    "    return labels2_fid, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read or generate data\n",
    "# this data is the 2scale toy example in the papaer\n",
    "\n",
    "def give_circle_points(rmin, rmax, n_points, center):\n",
    "    # This function generates \"n_points\" uniform points inside a circle with center of \"center\", \n",
    "    # and a random radius between \"r_min\" and \"rmax\"\n",
    "    r         = np.random.uniform(rmin, rmax, 1)\n",
    "    r_s       = np.random.uniform(0, r, n_points)\n",
    "    theta_s   = np.random.uniform(0, np.pi*2, n_points)\n",
    "    sin_theta = np.sin(theta_s)\n",
    "    cos_theta = np.cos(theta_s)\n",
    "    points    = [[center[0] + r_s[i] * cos_theta[i], center[1] + r_s[i] * sin_theta[i]] for i in range(n_points)]  \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_cluster(r, n_points, center):\n",
    "    r_s       = np.random.uniform(0, r, n_points)\n",
    "    theta_s   = np.random.uniform(0, np.pi*2, n_points)\n",
    "    sin_theta = np.sin(theta_s)\n",
    "    cos_theta = np.cos(theta_s)\n",
    "    points    = [[center[0] + r_s[i] * cos_theta[i], center[1] + r_s[i] * sin_theta[i]] for i in range(n_points)]  \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_cluster(sigma, n_points, center):\n",
    "    r_s       = np.random.normal(0, sigma, n_points)\n",
    "    theta_s   = np.random.uniform(0, np.pi*2, n_points)\n",
    "    sin_theta = np.sin(theta_s)\n",
    "    cos_theta = np.cos(theta_s)\n",
    "    points    = [[center[0] + r_s[i] * cos_theta[i], center[1] + r_s[i] * sin_theta[i]] for i in range(n_points)]  \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elliptical_cluster(a, b, R_matrix, n_points, center):\n",
    "    a_s = np.random.uniform(0, a, n_points)\n",
    "    b_s = np.random.uniform(0, b, n_points)\n",
    "    t_s = np.random.uniform(0, 2*math.pi, n_points)\n",
    "    sin_t = np.sin(t_s)\n",
    "    cos_t = np.cos(t_s)\n",
    "    temp =  [np.dot(R_matrix, np.asarray([a_s[i] * cos_t[i], b_s[i] * sin_t[i]])) for i in range(n_points)]\n",
    "    \n",
    "    points    = [[center[0] + t[0], center[1] + t[1]] for t in temp]  \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calling_graphic(data, sigma_s = 30.0, noise_thresh = 0.5):\n",
    "    # This funciton !! DESSCRIBE THE FUNCTION!! \n",
    "    \n",
    "    P1 = data[['x', 'y']].to_numpy() # select the coordinates columns, and transform into a numpy object\n",
    "    truelabels = data['labels_1'].to_numpy()\n",
    "    \n",
    "    matrices_str = data['cov_matrix']\n",
    "    Sigma1 = np.zeros((len(matrices_str), 2,2))\n",
    "    \n",
    "    for i, matrix_str in enumerate(matrices_str): \n",
    "        Sigma1[i,:,:] = np.array(matrix_str)\n",
    "        \n",
    "    n1 = data['N_photons'].to_numpy()\n",
    "    \n",
    "    density1, noise_points1, not_noise_points1, G0 = denoise(P1, Sigma1, sigma_s, noise_thresh, 1)\n",
    "    labels1, G1, k1 = cluster_single_scale(P1, Sigma1, not_noise_points1, sigma_s, 1)\n",
    "    #plot_the_graph(G0, P1, 'Delaunay Graph', 1)\n",
    "    #plot_the_graph(G1, P1[not_noise_points1], 'Delaunay Graph', 1)\n",
    "    labels1_fid     = labels1\n",
    "    #plot_points(P1, labels1, 'First scale - ' + str(k1) + ' clusters', 2)\n",
    "    #plot_clusters_with_graph(G1, P1[not_noise_points1], labels1_fid[not_noise_points1], '1st scale clusters with graph', 1)\n",
    "    labels1_fid += 1 # to match the convention\n",
    "    return labels1_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_clusters(labels, min_size = 3):\n",
    "    # This function removes the clusters made of n points, n< min_size, and the points become isolated points.\n",
    "    # Labels can either be a pandas Series, or a numpy array or a list.\n",
    "    ClusterLabToRemove = []\n",
    "    if type(labels) != pd.Series:\n",
    "        labels = pd.Series(labels)\n",
    "    for clus in labels.unique():\n",
    "            if clus != 0: # 0 label is noise \n",
    "                n_points = len(labels[(labels==clus)]) \n",
    "                if n_points < min_size:\n",
    "                    ClusterLabToRemove.append(clus)\n",
    "    # All points that are labelled with something in the ClusterLabToRemove list are re-labelled by 0, 0 = noise\n",
    "    labels = labels.replace(ClusterLabToRemove, 0)\n",
    "    return labels.to_numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_cov_matrix_format(data):\n",
    "    matrices = data['cov_matrix']\n",
    "    if type((matrices).iloc[0]) == str:\n",
    "        temp = [np.array(eval(matrix_in_string)) for matrix_in_string in matrices]\n",
    "        data['cov_matrix'] = temp\n",
    "    \n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
