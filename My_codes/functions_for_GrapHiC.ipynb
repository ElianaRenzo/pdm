{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from sklearn import metrics\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(P, labels, plot_tile, s) :\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    plt.scatter(P[:,0], P[:,1], c = labels, cmap = 'rainbow_r', s = s)\n",
    "    plt.gca().set_aspect('equal'), plt.title(plot_tile), plt.xlabel('x(mm)'), plt.ylabel('y(mm)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_graph(G, P, plot_title, node_size):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    nx.draw(G, P, node_size=[node_size for v in range(P.shape[0])], node_color='lightgreen'\n",
    "            ,edgelist=edges, edge_color=weights, width=5, edge_cmap=plt.cm.Blues)\n",
    "    plt.title(plot_title)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_with_graph(G, P, labels, plot_title, node_size):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    edges, weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    nodes = [i for i in G.nodes()]\n",
    "    e = []\n",
    "    w = []\n",
    "\n",
    "    for i, edge in enumerate(edges):\n",
    "        p1 = edge[0]\n",
    "        p2 = edge[1]\n",
    "        if labels[p1] == labels[p2] :\n",
    "            e.append((p1, p2))\n",
    "            w.append(1000)\n",
    "\n",
    "    nx.draw(G, P, node_size=[node_size for v in range(P.shape[0])], node_color=labels[nodes], \n",
    "            cmap=plt.cm.rainbow_r ,edgelist=e, edge_color=w, width=1)\n",
    "    plt.title(plot_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(P, Sigma, sigma_s, noise_thrsh, plot_den_hist) :\n",
    "    # Inputs : \n",
    "    # P_i           : [x, y], the localization centers\n",
    "    # Sigma_i       : [[sigma_x_i, 0], [0, sigma_y_i]] where sigma_x_i and sigma_y_i are \n",
    "    # uncertinties in x and y direction  of the P_i localization respectively.\n",
    "    # sigma_s       : scaling parameter\n",
    "    # noise_thrsh   : threshold used for denoising\n",
    "    # plot_den_hist : 0 or 1, 1 for when you want the density histogram plot\n",
    "    \n",
    "    n_points = P.shape[0]               # number of points\n",
    "    sg2      = sigma_s ** 2 \n",
    "    \n",
    "    # ------- Constructing weighted graph -------\n",
    "    dt = Delaunay(points=P)             # Delaunay triangulation for input points\n",
    "    G  = nx.Graph()                     # G will be the weighted graph for the data\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    for path in dt.simplices:\n",
    "        a  = path[0]\n",
    "        b  = path[1]\n",
    "        c  = path[2]\n",
    "        d1 = np.exp(-(distance.euclidean(P[a], P[b])**2 + np.trace(Sigma[a] + Sigma[b]))/(2*sg2))\n",
    "        d2 = np.exp(-(distance.euclidean(P[a], P[c])**2 + np.trace(Sigma[a] + Sigma[c]))/(2*sg2))\n",
    "        d3 = np.exp(-(distance.euclidean(P[b], P[c])**2 + np.trace(Sigma[b] + Sigma[c]))/(2*sg2))\n",
    "        G.add_weighted_edges_from([(a, b, d1)])\n",
    "        G.add_weighted_edges_from([(a, c, d2)])\n",
    "        G.add_weighted_edges_from([(b, c, d3)])\n",
    "    \n",
    "    G0 = G.copy()\n",
    "    # ------- Denosing -------\n",
    "    degrees          = G.degree(weight=None)\n",
    "    n_neighbors      = np.array([i for (_,i) in degrees]).ravel()\n",
    "    weights          = G.degree(weight='weight')\n",
    "    weighted_degrees = np.array([i for (_,i) in weights]).ravel()\n",
    "    density          = np.nan_to_num(weighted_degrees/n_neighbors)\n",
    "\n",
    "    not_noise_points = np.where(density>noise_thrsh)[0]\n",
    "    noise_points     = np.where(density<=noise_thrsh)[0]\n",
    "    \n",
    "    # ------- Denoising with kmeans - not in the paper -------\n",
    "#     den_kmeans = KMeans(n_clusters=3, random_state=0, n_init = 20).fit(density.reshape(-1, 1))\n",
    "#     den_groups       = den_kmeans.labels_\n",
    "#     den_groups_means = np.array([np.mean(density[np.where(den_groups==i)]) for i in range(3)])\n",
    "#     min_density      = np.where(den_groups==min(den_groups))[0]\n",
    "#     not_noise_points = np.where(den_groups!=min_density)[0]\n",
    "#     noise_points     = np.where(den_groups==min_density)[0]\n",
    "    \n",
    "    if plot_den_hist:\n",
    "        plt.hist(density, bins=100)\n",
    "        plt.title('histogram of node density -all-')\n",
    "        plt.show()\n",
    "        \n",
    "    return density, noise_points, not_noise_points, G0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels : set at domain decomposition step\n",
    "\n",
    "def cluster_single_scale(P, Sigma, not_noise_point, sigma_s, plot_eigen) :\n",
    "    # Inputs : \n",
    "    # P_i : [x, y], the localization centers\n",
    "    # Sigma_i : [[sigma_x_i, 0], [0, sigma_y_i]] where sigma_x_i and sigma_y_i are \n",
    "    # uncertinties in x and y direction  of the P_i localization respectively.\n",
    "    # sigma_s : scaling parameter\n",
    "    # not_noise_point : index of not noise points\n",
    "    # plot_eigen : 0 or 1, 1 if you want eigenvalues and eigenvalue differences plot\n",
    "\n",
    "    n_points_wn = P.shape[0]\n",
    "    P           = P[not_noise_point]\n",
    "    Sigma       = Sigma[not_noise_point]\n",
    "    n_points    = P.shape[0]               # number of non-noise points\n",
    "    sg2         = sigma_s ** 2 \n",
    "    \n",
    "    if n_points < 3 :\n",
    "        print('Not enough points!')\n",
    "        return\n",
    "    \n",
    "    # ------- Constructing weighted graph -------\n",
    "    dt = Delaunay(points=P)             # Delaunay triangulation for input points\n",
    "    G  = nx.Graph()                     # G will be the weighted graph for the data\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    for path in dt.simplices:\n",
    "        a  = path[0]\n",
    "        b  = path[1]\n",
    "        c  = path[2]\n",
    "        d1 = np.exp(-(distance.euclidean(P[a], P[b])**2 + np.trace(Sigma[a] + Sigma[b]))/(2*sg2))\n",
    "        d2 = np.exp(-(distance.euclidean(P[a], P[c])**2 + np.trace(Sigma[a] + Sigma[c]))/(2*sg2))\n",
    "        d3 = np.exp(-(distance.euclidean(P[b], P[c])**2 + np.trace(Sigma[b] + Sigma[c]))/(2*sg2))\n",
    "        G.add_weighted_edges_from([(a, b, d1)])\n",
    "        G.add_weighted_edges_from([(a, c, d2)])\n",
    "        G.add_weighted_edges_from([(b, c, d3)])\n",
    "        \n",
    "    # ------- Spectral Clustering -------\n",
    "    \n",
    "    # ------- 1. Calculating eigenvalues and eigenvectors -------\n",
    "    vals, vecs = eigsh(nx.laplacian_matrix(G), k=n_points-1, which='SM')\n",
    "                    \n",
    "    # ------- 2. Determining number of clusters -------\n",
    "    \n",
    "    diff_vals  = np.diff(vals[0:int(n_points/2)])\n",
    "    prominence = np.max(diff_vals) / 4 # je crois que ici c'est le threshold pour renvoyer toutes les petites à 0.\n",
    "    #prominence = np.max(diff_vals) / 8 # je crois que ici c'est le threshold pour renvoyer toutes les petites à 0.\n",
    "    k          = np.min(np.where(diff_vals >= prominence)[0]) + 1\n",
    "    \n",
    "    if vals[1] > prominence:   # handle only 1 cluster case\n",
    "        k = 1\n",
    "        \n",
    "    if plot_eigen:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "        axs[0].scatter(list(range(len(diff_vals[0:k+5]))), diff_vals[0:k+5])\n",
    "        axs[0].set_title('Eigenvalue Differences')\n",
    "        axs[1].scatter(list(range(len(vals[0:k+5]))), vals[0:k+5])\n",
    "        axs[1].set_title('Eigenvalues')\n",
    "        plt.show()\n",
    "        \n",
    "    # ------- 3. Kmeans -------\n",
    "    kmeans_spec = KMeans(n_clusters=k, random_state=0, n_init = 20).fit(vecs[:,0:k])\n",
    "    labels_not_noise = kmeans_spec.labels_\n",
    "    \n",
    "    labels                  = np.zeros((n_points_wn, 1)).ravel() - 1 # -1 is label for noise\n",
    "    labels[not_noise_point] = labels_not_noise\n",
    "    \n",
    "    return labels, G, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_to_point(P, Sigma, n, k, labels) :\n",
    "    # This function turns clusters into their respresentitive point of the next scale\n",
    "    # Inputs : \n",
    "    # P_i : [x, y], the localization centers\n",
    "    # Sigma_i : [[sigma_x_i, 0], [0, sigma_y_i]] where sigma_x_i and sigma_y_i are \n",
    "    # uncertinties in x and y direction  of the P_i localization respectively.\n",
    "    # n_i : number of photons in P_i th detection\n",
    "    \n",
    "    P_next      = np.zeros((k, 2))\n",
    "    Sigma_next  = np.zeros((k, 2, 2))\n",
    "    n_next      = np.zeros((k, 1)).ravel()\n",
    "    cluster_id  = np.zeros((k, 1))\n",
    "\n",
    "    for i, cluster_num in enumerate(np.unique(labels)):\n",
    "        idx_cluster = np.where(labels==cluster_num)[0]\n",
    "        mx          = np.average(P[idx_cluster, 0], axis=0, weights=n[idx_cluster])\n",
    "        my          = np.average(P[idx_cluster, 1], axis=0, weights=n[idx_cluster])\n",
    "        sx          = 0\n",
    "        sy          = 0\n",
    "        sxy         = 0\n",
    "        \n",
    "        for idx in idx_cluster : \n",
    "            sx  = (P[idx,0]-mx) ** 2 * n[idx] + sx\n",
    "            sy  = (P[idx,1]-my) ** 2 * n[idx] + sy\n",
    "            sxy = (P[idx,0]-mx) * (P[idx,1]-my) * n[idx] + sxy\n",
    "            \n",
    "        sx  = sx  / (np.sum(n[idx_cluster]))\n",
    "        sy  = sy  / (np.sum(n[idx_cluster]))\n",
    "        sxy = sxy / (np.sum(n[idx_cluster]))\n",
    "        \n",
    "        P_next[i]      = [mx, my]\n",
    "        Sigma_next[i]  = [[sx, sxy], [sxy, sy]]\n",
    "        n_next[i]      = np.sum(n[idx_cluster])\n",
    "        cluster_id[i]  = cluster_num \n",
    "        \n",
    "    return P_next, Sigma_next, n_next, cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_input(labels1_fid, labels2, k2, cluster_id2):\n",
    "    # This function maps clustering labels of a specific scale data to the input data\n",
    "    # Inputs:\n",
    "    # labels1_fid : labels of the previous scale for the input data\n",
    "    # labels2 : labels of data clustered at the current scale\n",
    "    # k2 : number of the clusters in the current scale\n",
    "\n",
    "    labels2_fid     = labels1_fid.copy()\n",
    "    counter         = k2\n",
    "    \n",
    "    for lbl in np.unique(labels2):\n",
    "        idx_ns = cluster_id2[np.where(labels2==lbl)[0]]\n",
    "\n",
    "        if lbl == -1:\n",
    "            for idx in idx_ns:\n",
    "                idx_fs              = np.where(labels1_fid==idx)[0]\n",
    "                labels2_fid[idx_fs] = counter\n",
    "                counter = counter + 1\n",
    "\n",
    "        else:\n",
    "            for idx in idx_ns:\n",
    "                idx_fs              = np.where(labels1_fid==idx)[0]\n",
    "                labels2_fid[idx_fs] = lbl\n",
    "            \n",
    "    return labels2_fid, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read or generate data\n",
    "# this data is the 2scale toy example in the papaer\n",
    "\n",
    "def give_circle_points(rmin, rmax, n_points, center):\n",
    "    # This function generates \"n_points\" uniform points inside a circle with center of \"center\", \n",
    "    # and a random radius between \"r_min\" and \"rmax\"\n",
    "    r         = np.random.uniform(rmin, rmax, 1)\n",
    "    r_s       = np.random.uniform(0, r, n_points)\n",
    "    theta_s   = np.random.uniform(0, np.pi*2, n_points)\n",
    "    sin_theta = np.sin(theta_s)\n",
    "    cos_theta = np.cos(theta_s)\n",
    "    points    = [[center[0] + r_s[i] * cos_theta[i], center[1] + r_s[i] * sin_theta[i]] for i in range(n_points)]  \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
